---
apiVersion: karpenter.sh/v1beta1
kind: NodePool # Previously kind: Provisioner
metadata:
  name: spark-compute-optimized
  namespace: karpenter # Same namespace as Karpenter add-on installed
spec:
  template:
    metadata:
      labels:
        type: karpenter
        provisioner: spark-compute-optimized
        NodeGroupType: SparkComputeOptimized
    spec:
      nodeClassRef:
        name: spark-compute-optimized
      requirements:
        - key: "topology.kubernetes.io/zone"
          operator: In
          values: [${azs}a] #Update the correct region and zones
        - key: "karpenter.sh/capacity-type"
          operator: In
          values: ["spot", "on-demand"]
        - key: "kubernetes.io/arch"
          operator: In
          values: ["amd64"]
        - key: "karpenter.k8s.aws/instance-category"
          operator: In
          values: ["c"]
        - key: "karpenter.k8s.aws/instance-family"
          operator: In
          values: ["c5d"]
        - key: "karpenter.k8s.aws/instance-cpu"
          operator: In
          values: ["4", "8", "16", "36"]
        - key: "karpenter.k8s.aws/instance-hypervisor"
          operator: In
          values: ["nitro"]
        - key: "karpenter.k8s.aws/instance-generation"
          operator: Gt
          values: ["2"]
  limits:
    cpu: 1000
  disruption:
    # Describes which types of Nodes Karpenter should consider for consolidation
    # If using 'WhenUnderutilized', Karpenter will consider all nodes for consolidation and attempt to remove or replace Nodes when it discovers that the Node is underutilized and could be changed to reduce cost
    # If using `WhenEmpty`, Karpenter will only consider nodes for consolidation that contain no workload pods
    consolidationPolicy: WhenEmpty
    # The amount of time Karpenter should wait after discovering a consolidation decision
    # This value can currently only be set when the consolidationPolicy is 'WhenEmpty'
    # You can choose to disable consolidation entirely by setting the string value 'Never' here
    consolidateAfter: 30s
    # The amount of time a Node can live on the cluster before being removed
    # Avoiding long-running Nodes helps to reduce security vulnerabilities as well as to reduce the chance of issues that can plague Nodes with long uptimes such as file fragmentation or memory leaks from system processes
    # You can choose to disable expiration entirely by setting the string value 'Never' here
    expireAfter: 720h

  # Priority given to the NodePool when the scheduler considers which NodePool
  # to select. Higher weights indicate higher priority when comparing NodePools.
  # Specifying no weight is equivalent to specifying a weight of 0.
  weight: 10



# NOTE: Multiple NodePools may point to the same EC2NodeClass.
---
apiVersion: karpenter.k8s.aws/v1beta1
kind: EC2NodeClass # Previously kind: AWSNodeTemplate
metadata:
  name: spark-compute-optimized
  namespace: karpenter
spec:
  amiFamily: AL2
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 50Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true
  # required, IAM role to use for the node identity
  role: "${ec2_node_role}"
  subnetSelectorTerms:
    - tags: # Update the correct region and zones
        Name: "${eks_cluster_id}/*/Private*"   	#emr-roadshow/network-sg/eksVpc/Private
  securityGroupSelectorTerms:
    - name: "eks-*-${eks_cluster_id}-*"
  userData: |                   
    MIME-Version: 1.0
    Content-Type: multipart/mixed; boundary="BOUNDARY"

    --BOUNDARY
    Content-Type: text/x-shellscript; charset="us-ascii"

    #!/bin/bash
    echo "Running a custom user data script"
    set -ex
    yum install mdadm -y

    DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}')

    DISK_ARRAY=()

    for DEV in $DEVICES
    do
      DISK_ARRAY+=("/dev/$${DEV}")
    done

    DISK_COUNT=$${#DISK_ARRAY[@]}

    if [ $${DISK_COUNT} -eq 0 ]; then
      echo "No SSD disks available. No further action needed."
    else
      if [ $${DISK_COUNT} -eq 1 ]; then
        TARGET_DEV=$${DISK_ARRAY[0]}
        mkfs.xfs $${TARGET_DEV}
      else
        mdadm --create --verbose /dev/md0 --level=0 --raid-devices=$${DISK_COUNT} $${DISK_ARRAY[@]}
        mkfs.xfs /dev/md0
        TARGET_DEV=/dev/md0
      fi

      mkdir -p /local1
      echo $${TARGET_DEV} /local1 xfs defaults,noatime 1 2 >> /etc/fstab
      mount -a
      /usr/bin/chown -hR +999:+1000 /local1
    fi

    --BOUNDARY--
  tags:
    InstanceType: "spark-compute-optimized"    # optional, add tags for your own use