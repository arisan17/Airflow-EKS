apiVersion: karpenter.sh/v1alpha5
kind: Provisioner
metadata:
  name: spark-compute-optimized
  namespace: karpenter # Same namespace as Karpenter add-on installed
spec:
  kubeletConfiguration:
    containerRuntime: containerd
    #    podsPerCore: 2
    #    maxPods: 20
  requirements:
    - key: "topology.kubernetes.io/zone"
      operator: In
      values: [${azs}a] #Update the correct region and zones
    - key: "karpenter.sh/capacity-type"
      operator: In
      values: ["spot", "on-demand"]
    - key: "node.kubernetes.io/instance-type" #If not included, all instance types are considered
      operator: In
      values: ["c5d.large","c5d.xlarge","c5d.2xlarge","c5d.4xlarge","c5d.9xlarge"] # 1 NVMe disk
    - key: "kubernetes.io/arch"
      operator: In
      values: ["amd64"]
  limits:
    resources:
      cpu: 1000
  providerRef:
    name: spark-compute-optimized
  labels:
    type: karpenter
    provisioner: spark-compute-optimized
    NodeGroupType: SparkComputeOptimized
  taints:
    - key: spark-compute-optimized
      value: 'true'
      effect: NoSchedule
  ttlSecondsAfterEmpty: 120 # optional, but never scales down if not set

---
apiVersion: karpenter.k8s.aws/v1alpha1
kind: AWSNodeTemplate
metadata:
  name: spark-compute-optimized
  namespace: karpenter
spec:
  blockDeviceMappings:
    - deviceName: /dev/xvda
      ebs:
        volumeSize: 100Gi
        volumeType: gp3
        encrypted: true
        deleteOnTermination: true
  metadataOptions:
    httpEndpoint: enabled
    httpProtocolIPv6: disabled
    httpPutResponseHopLimit: 2
    httpTokens: required
  subnetSelector:
    Name: "${eks_cluster_id}-private*"        # Name of the Subnets to spin up the nodes
  securityGroupSelector:                      # required, when not using launchTemplate
    Name: "${eks_cluster_id}-node*"           # name of the SecurityGroup to be used with Nodes
  #  instanceProfile: ""      # optional, if already set in controller args
  userData: |
    MIME-Version: 1.0
    Content-Type: multipart/mixed; boundary="BOUNDARY"

    --BOUNDARY
    Content-Type: text/x-shellscript; charset="us-ascii"

    #!/bin/bash
    echo "Running a custom user data script"
    set -ex

    IDX=1
    DEVICES=$(lsblk -o NAME,TYPE -dsn | awk '/disk/ {print $1}')

    for DEV in $DEVICES
    do
      mkfs.xfs /dev/$${DEV}
      mkdir -p /local$${IDX}
      echo /dev/$${DEV} /local$${IDX} xfs defaults,noatime 1 2 >> /etc/fstab
      IDX=$(($${IDX} + 1))
    done

    mount -a

    /usr/bin/chown -hR +999:+1000 /local*

    --BOUNDARY--

  tags:
    InstanceType: "spark-compute-optimized"    # optional, add tags for your own use
